{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abf6f5f-3396-4285-89ec-4b1709f6553c",
   "metadata": {},
   "source": [
    "# MobileBERT for Question Answering on the SQuAD dataset\n",
    "\n",
    "### 3. Evaluating the fine-tuned model on the validation set \n",
    "\n",
    "In these notebooks we are going use [MobileBERT implemented by HuggingFace](https://huggingface.co/docs/transformers/model_doc/mobilebert) on the question answering task by text-extraction on the [The Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/). The data is composed by a set of questions and paragraphs that contain the answers. The model will be trained to locate the answer in the context by giving the positions where the answer starts and ends.\n",
    "\n",
    "In this notebook we are going to evaluate the model from a checkpoint we obtained in the fine-tuning step.\n",
    "\n",
    "More info from HuggingFace docs:\n",
    "- [Question Answering](https://huggingface.co/tasks/question-answering)\n",
    "- [Glossary](https://huggingface.co/transformers/glossary.html#model-inputs)\n",
    "- [Question Answering chapter of NLP course](https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a9fef8-4780-4779-89de-662eb014d8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, MobileBertForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08765c80-6338-4dfa-97ce-cdd5adbc26af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils import disable_progress_bar\n",
    "from datasets import disable_caching\n",
    "\n",
    "\n",
    "disable_progress_bar()\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f44b4b-935a-4f86-b4fc-87d2036fd215",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_checkpoint = 'google/mobilebert-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e9e77f-17f5-4f0d-ace5-f26fa74b3c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForQuestionAnswering were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MobileBertForQuestionAnswering.from_pretrained(hf_model_checkpoint)\n",
    "\n",
    "model.eval();\n",
    "\n",
    "# use checkpoint from fine-tuning\n",
    "model.load_state_dict(\n",
    "    torch.load('mobilebertqa_ft_final',\n",
    "               map_location=torch.device('cpu')\n",
    "              )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728089ed-5d62-491a-b623-d6928df88823",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0711c779-f6d0-4c6b-b127-286d8a4225fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ebb191-90f2-48e9-bf6c-80982682a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "# Find more info about this in the notebook about exploring the dataset\n",
    "\n",
    "MAX_SEQ_LEN = 300\n",
    "\n",
    "def tokenize_dataset(squad_example, tokenizer=tokenizer):\n",
    "    \"\"\"Tokenize the text in the dataset and convert\n",
    "    the start and ending positions of the answers\n",
    "    from text to tokens\"\"\"\n",
    "    max_len = MAX_SEQ_LEN\n",
    "    context = squad_example['context']\n",
    "    answer_start = squad_example['answers']['answer_start'][0]\n",
    "    answer = squad_example['answers']['text'][0]\n",
    "    squad_example_tokenized = tokenizer(\n",
    "        context, squad_example['question'],\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        truncation='only_first',\n",
    "    )\n",
    "    token_start = len(tokenizer.tokenize(context[:answer_start + 1]))\n",
    "    token_end = len(tokenizer.tokenize(answer)) + token_start\n",
    "\n",
    "    squad_example_tokenized['start_token_idx'] = token_start\n",
    "    squad_example_tokenized['end_token_idx'] = token_end\n",
    "\n",
    "    return squad_example_tokenized\n",
    "\n",
    "\n",
    "def filter_samples_by_max_seq_len(squad_example):\n",
    "    \"\"\"Fliter out the samples where the answers are\n",
    "    not within the first `MAX_SEQ_LEN` tokens\"\"\"\n",
    "    max_len = MAX_SEQ_LEN\n",
    "    answer_start = squad_example['answers']['answer_start'][0]\n",
    "    answer = squad_example['answers']['text'][0]\n",
    "    token_start = len(tokenizer.tokenize(squad_example['context'][:answer_start]))\n",
    "    token_end = len(tokenizer.tokenize(answer)) + token_start\n",
    "    return token_end < max_len\n",
    "\n",
    "dataset_filtered = hf_dataset['validation'].filter(\n",
    "    filter_samples_by_max_seq_len,\n",
    "    num_proc=24,\n",
    ")\n",
    "\n",
    "dataset_tok = dataset_filtered.map(\n",
    "    tokenize_dataset,\n",
    "    remove_columns=hf_dataset['validation'].column_names,\n",
    "    num_proc=12,\n",
    ")\n",
    "dataset_tok.set_format('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c871cd-568c-4746-b26f-4fb8fecfb475",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(\n",
    "    dataset_tok,\n",
    "    shuffle=True,   # shuffle to print different predictions\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2efef42e-f72a-4f69-9545-96ef7e99b54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 4.53kB [00:00, 11.7MB/s]\n",
      "Downloading extra modules: 3.32kB [00:00, 8.60MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Define a metric that tell us how good the preductions are\n",
    "squad_metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a5e61e3-df88-4316-8bbf-c1f3ab619a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* in 1975, season 11 of the series won a writers'guild of great britain award for best writing in a children's serial. in 1996, bbc television held the \" auntie awards \" as the culmination of their \" tv60 \" series, celebrating 60 years of bbc television broadcasting, where doctor who was voted as the \" best popular drama \" the corporation had ever produced, ahead of such ratings heavyweights as eastenders and casualty. in 2000, doctor who was ranked third in a list of the 100 greatest british television programmes of the 20th century, produced by the british film institute and voted on by industry professionals. in 2005, the series came first in a survey by sfx magazine of \" the greatest uk science fiction and fantasy television series ever \". also, in the 100 greatest kids'tv shows ( a channel 4 countdown in 2001 ), the 1963 â€“ 1989 run was placed at number eight. what rank does doctor who hold in a list of the 100 greatest kids'tv shows?\n",
      "\n",
      "[  model  ] eight\n",
      "[   ref   ] eight\n",
      "[ metrics ] {'exact_match': 100.0, 'f1': 100.0}\n",
      "\n",
      "---\n",
      "\n",
      "* in early 2012, nfl commissioner roger goodell stated that the league planned to make the 50th super bowl \" spectacular \" and that it would be \" an important game for us as a league \". what year did roger goodell announce that super bowl 50 would be \" important \"?\n",
      "\n",
      "[  model  ] early 2012\n",
      "[   ref   ] 2012\n",
      "[ metrics ] {'exact_match': 0.0, 'f1': 66.66666666666666}\n",
      "\n",
      "---\n",
      "\n",
      "* the party, or parties, that hold the majority of seats in the parliament forms the scottish government. in contrast to many other parliamentary systems, parliament elects a first minister from a number of candidates at the beginning of each parliamentary term ( after a general election ). any member can put their name forward to be first minister, and a vote is taken by all members of parliament. normally, the leader of the largest party is returned as first minister, and head of the scottish government. theoretically, parliament also elects the scottish ministers who form the government of scotland and sit in the scottish cabinet, but such ministers are, in practice, appointed to their roles by the first minister. junior ministers, who do not attend cabinet, are also appointed to assist scottish ministers in their departments. most ministers and their juniors are drawn from amongst the elected msps, with the exception of scotland's chief law officers : the lord advocate and the solicitor general. whilst the first minister chooses the ministers â€“ and may decide to remove them at any time â€“ the formal appointment or dismissal is made by the sovereign. who makes formal appointment or dismissal decisions?\n",
      "\n",
      "[  model  ] the sovereign\n",
      "[   ref   ] the sovereign\n",
      "[ metrics ] {'exact_match': 100.0, 'f1': 100.0}\n",
      "\n",
      "---\n",
      "\n",
      "* between 1991 and 2000, the total area of forest lost in the amazon rose from 415, 000 to 587, 000 square kilometres ( 160, 000 to 227, 000 sq mi ), with most of the lost forest becoming pasture for cattle. seventy percent of formerly forested land in the amazon, and 91 % of land deforested since 1970, is used for livestock pasture. currently, brazil is the second - largest global producer of soybeans after the united states. new research however, conducted by leydimere oliveira et al., has shown that the more rainforest is logged in the amazon, the less precipitation reaches the area and so the lower the yield per hectare becomes. so despite the popular perception, there has been no economical advantage for brazil from logging rainforest zones and converting these to pastoral fields. how many square kilometres of the amazon forest was lost by 1991?\n",
      "\n",
      "[  model  ] 415, 000 to 587, 000\n",
      "[   ref   ] 415, 000\n",
      "[ metrics ] {'exact_match': 0.0, 'f1': 57.14285714285715}\n",
      "\n",
      "---\n",
      "\n",
      "* the panthers finished the regular season with a 15 â€“ 1 record, and quarterback cam newton was named the nfl most valuable player ( mvp ). they defeated the arizona cardinals 49 â€“ 15 in the nfc championship game and advanced to their second super bowl appearance since the franchise was founded in 1995. the broncos finished the regular season with a 12 â€“ 4 record, and denied the new england patriots a chance to defend their title from super bowl xlix by defeating them 20 â€“ 18 in the afc championship game. they joined the patriots, dallas cowboys, and pittsburgh steelers as one of four teams that have made eight appearances in the super bowl. how many teams have been in the super bowl eight times?\n",
      "\n",
      "[  model  ] four\n",
      "[   ref   ] four\n",
      "[ metrics ] {'exact_match': 100.0, 'f1': 100.0}\n",
      "\n",
      "---\n",
      "\n",
      "* plants have two main immune responses â€” the hypersensitive response, in which infected cells seal themselves off and undergo programmed cell death, and systemic acquired resistance, where infected cells release signals warning the rest of the plant of a pathogen's presence. chloroplasts stimulate both responses by purposely damaging their photosynthetic system, producing reactive oxygen species. high levels of reactive oxygen species will cause the hypersensitive response. the reactive oxygen species also directly kill any pathogens within the cell. lower levels of reactive oxygen species initiate systemic acquired resistance, triggering defense - molecule production in the rest of the plant. how do chloroplasts trigger the plant's immune system?\n",
      "\n",
      "[  model  ] purposely damaging their photosynthetic system, producing reactive oxygen species. high levels of reactive oxygen species will cause the hypersensitive response. the reactive oxygen species also directly kill any pathogens within the cell. lower levels of reactive oxygen species initiate systemic acquired resistance\n",
      "[   ref   ] by purposely damaging their photosynthetic system\n",
      "[ metrics ] {'exact_match': 0.0, 'f1': 22.22222222222222}\n",
      "\n",
      "---\n",
      "\n",
      "* the problems with north american were severe enough in late 1965 to cause manned space flight administrator george mueller to appoint program director samuel phillips to head a \" tiger team \" to investigate north american's problems and identify corrections. phillips documented his findings in a december 19 letter to naa president lee atwood, with a strongly worded letter by mueller, and also gave a presentation of the results to mueller and deputy administrator robert seamans. meanwhile, grumman was also encountering problems with the lunar module, eliminating hopes it would be ready for manned flight in 1967, not long after the first manned csm flights. what year was the first manned flight with the lunar module scheduled?\n",
      "\n",
      "[  model  ] 1967\n",
      "[   ref   ] 1967\n",
      "[ metrics ] {'exact_match': 100.0, 'f1': 100.0}\n",
      "\n",
      "---\n",
      "\n",
      "* east and central africa's biggest economy has posted tremendous growth in the service sector, boosted by rapid expansion in telecommunication and financial activity over the last decade, and now [ when? ] contributes 62 % of gdp. 22 % of gdp still comes from the unreliable agricultural sector which employs 75 % of the labour force ( a consistent characteristic of under - developed economies that have not attained food security â€“ an important catalyst of economic growth ) a small portion of the population relies on food aid. [ citation needed ] industry and manufacturing is the smallest sector, accounting for 16 % of gdp. the service, industry and manufacturing sectors only employ 25 % of the labour force but contribute 75 % of gdp. what percent of the labor force work in agriculture?\n",
      "\n",
      "[  model  ] 75 %\n",
      "[   ref   ] 75 % of the labour force\n",
      "[ metrics ] {'exact_match': 0.0, 'f1': 40.0}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a few random samples\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    # evaluate the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )\n",
    "    \n",
    "    # obtain the predicted start and end possitions logits and apply\n",
    "    # a softmax to to them to get the probability distribution\n",
    "    start_distr = F.softmax(outputs.start_logits, dim=-1)\n",
    "    end_distr   = F.softmax(outputs.end_logits,   dim=-1)\n",
    "    \n",
    "    # loop over the batch of inputs and outputs\n",
    "    for context_tokens, start_ref, end_ref, start_pred, end_pred, in zip(batch['input_ids'],\n",
    "                                                               batch['start_token_idx'], batch['end_token_idx'],\n",
    "                                                               start_distr, end_distr):\n",
    "        # get back the text from the tokenizers since both the train and\n",
    "        # validation sets has been replaced by tokenized versions\n",
    "        # * This is also important for the metrics because the original\n",
    "        #   text may be different than the one recovered from the\n",
    "        #   tokens in terms of spaces around puntuation or certain\n",
    "        #   symbols. Will be working only with text recovered\n",
    "        #   from tokens\n",
    "        context_text = tokenizer.decode(context_tokens, skip_special_tokens=True,\n",
    "                                        clean_up_tokenization_spaces=True)\n",
    "\n",
    "        # find the max class that the softmax gives\n",
    "        start_pred = torch.argmax(start_pred)\n",
    "        end_pred = torch.argmax(end_pred)\n",
    "        \n",
    "        # predicted answer\n",
    "        answer_tokens = context_tokens[start_pred:end_pred]\n",
    "        answer_text = tokenizer.decode(answer_tokens, skip_special_tokens=True,\n",
    "                                       clean_up_tokenization_spaces=True)\n",
    "        start_text = len(tokenizer.decode(context_tokens[:start_pred],\n",
    "                                          skip_special_tokens=True,\n",
    "                                          clean_up_tokenization_spaces=True)) + 1\n",
    "        \n",
    "        # reference answers\n",
    "        answer_tokens_ref = context_tokens[start_ref:end_ref]\n",
    "        answer_text_ref = tokenizer.decode(answer_tokens_ref, skip_special_tokens=True,\n",
    "                                           clean_up_tokenization_spaces=True)\n",
    "\n",
    "        # metrics\n",
    "        predictions = [{'prediction_text': answer_text, 'id': 'xxx'}]\n",
    "        references = [{'answers': {'answer_start': [start_text], 'text': [answer_text_ref]}, 'id': 'xxx'}]\n",
    "        results = squad_metric.compute(predictions=predictions, references=references)\n",
    "                \n",
    "        print(f'* {context_text}\\n')\n",
    "        print(f'[  model  ] {answer_text}')\n",
    "        print(f'[   ref   ] {answer_text_ref}')\n",
    "        print(f'[ metrics ] {results}')\n",
    "        print('\\n---\\n')\n",
    "        \n",
    "    # Run only the first batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3142de-abc5-4f42-b380-10a02aab1eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
