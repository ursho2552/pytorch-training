{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abf6f5f-3396-4285-89ec-4b1709f6553c",
   "metadata": {},
   "source": [
    "# MobileBERT for Question Answering on the SQuAD dataset\n",
    "\n",
    "### 3. Evaluating the fine-tuned model on the validation set \n",
    "\n",
    "In these notebooks we are going use [MobileBERT implemented by HuggingFace](https://huggingface.co/docs/transformers/model_doc/mobilebert) on the question answering task by text-extraction on the [The Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/). The data is composed by a set of questions and paragraphs that contain the answers. The model will be trained to locate the answer in the context by giving the positions where the answer starts and ends.\n",
    "\n",
    "In this notebook we are going to evaluate the model from a checkpoint we obtained in the fine-tuning step.\n",
    "\n",
    "More info from HuggingFace docs:\n",
    "- [Question Answering](https://huggingface.co/tasks/question-answering)\n",
    "- [Glossary](https://huggingface.co/transformers/glossary.html#model-inputs)\n",
    "- [Question Answering chapter of NLP course](https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a9fef8-4780-4779-89de-662eb014d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, MobileBertForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08765c80-6338-4dfa-97ce-cdd5adbc26af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils import disable_progress_bar\n",
    "from datasets import disable_caching\n",
    "\n",
    "\n",
    "disable_progress_bar()\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f44b4b-935a-4f86-b4fc-87d2036fd215",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_checkpoint = 'google/mobilebert-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e9e77f-17f5-4f0d-ace5-f26fa74b3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileBertForQuestionAnswering.from_pretrained(hf_model_checkpoint)\n",
    "\n",
    "model.eval();\n",
    "\n",
    "# use checkpoint from fine-tuning\n",
    "model.load_state_dict(\n",
    "    torch.load('mobilebertqa_ft',\n",
    "               map_location=torch.device('cpu')\n",
    "              )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728089ed-5d62-491a-b623-d6928df88823",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711c779-f6d0-4c6b-b127-286d8a4225fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebb191-90f2-48e9-bf6c-80982682a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "# Find more info about this in the notebook about exploring the dataset\n",
    "\n",
    "MAX_SEQ_LEN = 300\n",
    "\n",
    "def tokenize_dataset(squad_example, tokenizer=tokenizer):\n",
    "    \"\"\"Tokenize the text in the dataset and convert\n",
    "    the start and ending positions of the answers\n",
    "    from text to tokens\"\"\"\n",
    "    max_len = MAX_SEQ_LEN\n",
    "    context = squad_example['context']\n",
    "    answer_start = squad_example['answers']['answer_start'][0]\n",
    "    answer = squad_example['answers']['text'][0]\n",
    "    squad_example_tokenized = tokenizer(\n",
    "        context, squad_example['question'],\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        truncation='only_first',\n",
    "    )\n",
    "    token_start = len(tokenizer.tokenize(context[:answer_start + 1]))\n",
    "    token_end = len(tokenizer.tokenize(answer)) + token_start\n",
    "\n",
    "    squad_example_tokenized['start_token_idx'] = token_start\n",
    "    squad_example_tokenized['end_token_idx'] = token_end\n",
    "\n",
    "    return squad_example_tokenized\n",
    "\n",
    "\n",
    "def filter_samples_by_max_seq_len(squad_example):\n",
    "    \"\"\"Fliter out the samples where the answers are\n",
    "    not within the first `MAX_SEQ_LEN` tokens\"\"\"\n",
    "    max_len = MAX_SEQ_LEN\n",
    "    answer_start = squad_example['answers']['answer_start'][0]\n",
    "    answer = squad_example['answers']['text'][0]\n",
    "    token_start = len(tokenizer.tokenize(squad_example['context'][:answer_start]))\n",
    "    token_end = len(tokenizer.tokenize(answer)) + token_start\n",
    "    return token_end < max_len\n",
    "\n",
    "dataset_filtered = hf_dataset['validation'].filter(\n",
    "    filter_samples_by_max_seq_len,\n",
    "    num_proc=24,\n",
    ")\n",
    "\n",
    "dataset_tok = dataset_filtered.map(\n",
    "    tokenize_dataset,\n",
    "    remove_columns=hf_dataset['validation'].column_names,\n",
    "    num_proc=12,\n",
    ")\n",
    "dataset_tok.set_format('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c871cd-568c-4746-b26f-4fb8fecfb475",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(\n",
    "    dataset_tok,\n",
    "    shuffle=True,   # shuffle to print different predictions\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efef42e-f72a-4f69-9545-96ef7e99b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a metric that tell us how good the preductions are\n",
    "squad_metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e61e3-df88-4316-8bbf-c1f3ab619a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a few random samples\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    # evaluate the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            token_type_ids=batch['token_type_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )\n",
    "    \n",
    "    # obtain the predicted start and end possitions logits and apply\n",
    "    # a softmax to to them to get the probability distribution\n",
    "    start_distr = F.softmax(outputs.start_logits, dim=-1)\n",
    "    end_distr   = F.softmax(outputs.end_logits,   dim=-1)\n",
    "    \n",
    "    # loop over the batch of inputs and outputs\n",
    "    for context_tokens, start_ref, end_ref, start_pred, end_pred, in zip(batch['input_ids'],\n",
    "                                                               batch['start_token_idx'], batch['end_token_idx'],\n",
    "                                                               start_distr, end_distr):\n",
    "        # get back the text from the tokenizers since both the train and\n",
    "        # validation sets has been replaced by tokenized versions\n",
    "        # * This is also important for the metrics because the original\n",
    "        #   text may be different than the one recovered from the\n",
    "        #   tokens in terms of spaces around puntuation or certain\n",
    "        #   symbols. Will be working only with text recovered\n",
    "        #   from tokens\n",
    "        context_text = tokenizer.decode(context_tokens, skip_special_tokens=True,\n",
    "                                        clean_up_tokenization_spaces=True)\n",
    "\n",
    "        # find the max class that the softmax gives\n",
    "        start_pred = torch.argmax(start_pred)\n",
    "        end_pred = torch.argmax(end_pred)\n",
    "        \n",
    "        # predicted answer\n",
    "        answer_tokens = context_tokens[start_pred:end_pred]\n",
    "        answer_text = tokenizer.decode(answer_tokens, skip_special_tokens=True,\n",
    "                                       clean_up_tokenization_spaces=True)\n",
    "        start_text = len(tokenizer.decode(context_tokens[:start_pred],\n",
    "                                          skip_special_tokens=True,\n",
    "                                          clean_up_tokenization_spaces=True)) + 1\n",
    "        \n",
    "        # reference answers\n",
    "        answer_tokens_ref = context_tokens[start_ref:end_ref]\n",
    "        answer_text_ref = tokenizer.decode(answer_tokens_ref, skip_special_tokens=True,\n",
    "                                           clean_up_tokenization_spaces=True)\n",
    "\n",
    "        # metrics\n",
    "        predictions = [{'prediction_text': answer_text, 'id': 'xxx'}]\n",
    "        references = [{'answers': {'answer_start': [start_text], 'text': [answer_text_ref]}, 'id': 'xxx'}]\n",
    "        results = squad_metric.compute(predictions=predictions, references=references)\n",
    "                \n",
    "        print(f'* {context_text}\\n')\n",
    "        print(f'[  model  ] {answer_text}')\n",
    "        print(f'[   ref   ] {answer_text_ref}')\n",
    "        print(f'[ metrics ] {results}')\n",
    "        print('\\n---\\n')\n",
    "        \n",
    "    # Run only the first batch\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
