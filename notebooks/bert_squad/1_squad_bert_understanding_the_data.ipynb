{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileBERT for Question Answering on the SQuAD dataset\n",
    "\n",
    "### 1. Understanding the SQuAD dataset \n",
    "\n",
    "In these notebooks we are going use [MobileBERT implemented by HuggingFace](https://huggingface.co/docs/transformers/model_doc/mobilebert) on the question answering task by text-extraction on the [The Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/). The data is composed by a set of questions and paragraphs that contain the answers. The model will be trained to locate the answer in the context by giving the positions where the answer starts and ends.\n",
    "\n",
    "In this notebook we are going to explore the dataset and see how to set it up for fine-tuning.\n",
    "\n",
    "More info from HuggingFace docs:\n",
    "- [Question Answering](https://huggingface.co/tasks/question-answering)\n",
    "- [Glossary](https://huggingface.co/transformers/glossary.html#model-inputs)\n",
    "- [Question Answering chapter of NLP course](https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from rich.pretty import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.utils import disable_progress_bar\n",
    "from datasets import disable_caching\n",
    "\n",
    "\n",
    "disable_progress_bar()\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "hf_dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataset to check how it is partitioned\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check five train set samples to see how they look\n",
    "for _squad_example in hf_dataset['train'].select(range(5)):\n",
    "    pprint(_squad_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's five validation sample to see how they look\n",
    "for _squad_example in hf_dataset['validation'].select(range(5)):\n",
    "    pprint(_squad_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual samples can be accessed as a dictionary\n",
    "squad_ex = hf_dataset['train'].select([20584])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_ex['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_ex['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_ex['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_ex['answers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data for training\n",
    "Now we process the data so we can feed it later to the model.\n",
    "The idea is to replace the words (and some word parts) by numbers using the tokenizer above and organize the training data as a set of paragraphs and questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will work with this model\n",
    "hf_model = 'google/mobilebert-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tokenizer that was used for pretraining that model\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "1. Check the `tokenizer` object and find out its vocabulary length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "There are a few preprocessing operations that we need to do in the dataset so it can be fed to the HuggingFace MobileBERT model class:\n",
    " 1. Tokenize the contexts and the answers with the tokenizer we extracted (it already outputs a dictionary in the shape the model expects)\n",
    " 2. Convert the start and ending positions from relative-to-character to relative-to-token. For example, in the string `\"the cat sat in the mat\"`, the answer for the question 'Where did the cat sit', starts at character 16. In the dataset it would appear as `{'answer_start': [16]}`. If the sentence is tokenized as `[\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]`, the answer starts at token 5.\n",
    " 3. Discard the question/context pairs where the the answer appears outside of the truncation lenght of the tokenizer. This will be done only to make the tutorial simpler, as it can result in loss of information and potentially impact the performance of the model. Instead of discarding the extra tokens, one can make a smaller contexts by removing the begining so the answers fit. Find more info in the [Question Answering chapter of HuggingFace's NLP course](https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length\n",
    "MAX_SEQ_LEN = 300\n",
    "\n",
    "def tokenize_dataset(squad_example, tokenizer=tokenizer):\n",
    "    \"\"\"Tokenize the text in the dataset and convert\n",
    "    the start and ending positions of the answers\n",
    "    from text to tokens\"\"\"\n",
    "    max_len = MAX_SEQ_LEN\n",
    "    context = squad_example['context']\n",
    "    answer_start = squad_example['answers']['answer_start'][0]\n",
    "    answer = squad_example['answers']['text'][0]\n",
    "    squad_example_tokenized = tokenizer(\n",
    "        context, squad_example['question'],\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        truncation='only_first',\n",
    "    )\n",
    "    token_start = len(tokenizer.tokenize(context[:answer_start + 1]))\n",
    "    token_end = len(tokenizer.tokenize(answer)) + token_start\n",
    "\n",
    "    # Add the \"start_token_idx\" and \"end_token_idx\" keys to the \n",
    "    # `squad_example_tokenized` dictionary\n",
    "    squad_example_tokenized['start_token_idx'] = token_start\n",
    "    squad_example_tokenized['end_token_idx'] = token_end\n",
    "\n",
    "    return squad_example_tokenized\n",
    "\n",
    "\n",
    "def filter_samples_by_max_seq_len(squad_example):\n",
    "    \"\"\"Fliter out the samples where the answers are\n",
    "    not within the first `MAX_SEQ_LEN` tokens\"\"\"\n",
    "    max_len = MAX_SEQ_LEN\n",
    "    answer_start = squad_example['answers']['answer_start'][0]\n",
    "    answer = squad_example['answers']['text'][0]\n",
    "    token_start = len(tokenizer.tokenize(squad_example['context'][:answer_start]))\n",
    "    token_end = len(tokenizer.tokenize(answer)) + token_start\n",
    "    return token_end < max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. In the function `tokenize_dataset`, what does the following code do? Try it outside of the function with one of the context and questions we extracted above. Make sure you understand all the arguments ;)\n",
    "```python\n",
    "    squad_example_tokenized = tokenizer(\n",
    "        context, squad_example['question'],\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        truncation='only_first',\n",
    "    )\n",
    "```\n",
    "2. Make sure you understand how `token_start` and `token_end` are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filtering function through a filter\n",
    "dataset_filtered = hf_dataset.filter(\n",
    "    filter_samples_by_max_seq_len,\n",
    "    num_proc=24,\n",
    ")\n",
    "\n",
    "# Display the dataset and compare with the original dataset\n",
    "# to see how many samples were filtered out\n",
    "dataset_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tokenizing function through a map\n",
    "# and remove the text-containing entries of the dataset\n",
    "dataset_tok = dataset_filtered.map(\n",
    "    tokenize_dataset,\n",
    "    remove_columns=hf_dataset['train'].column_names,\n",
    "    num_proc=24,\n",
    ")\n",
    "\n",
    "# Convert the internal format of the dataset to pytorch\n",
    "dataset_tok.set_format('pt')\n",
    "\n",
    "# Display the dataset and compare features with the\n",
    "# tokenized dataset\n",
    "dataset_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_tok[\"train\"]\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the sample 20299 of the training set to see how it looks\n",
    "train_sample = train_dataset.select([20299])[0]\n",
    "pprint(train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `input_ids` key (Question)\n",
    "\n",
    "1. What are the `\"input_ids\"` key in the tokenized dataset? Use `tokenizer.decode()` to \"de-tokenize\" back to text the sample `train_sample['input_ids']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `attention_mask` key\n",
    "The attention masks differentiate what is text and what is padding. More info [here](https://huggingface.co/transformers/glossary.html#attention-mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the padding tokens by evaluating the train sample on\n",
    "# train_sample['attention_mask'] == 1\n",
    "context_encoded = train_sample['input_ids'][train_sample['attention_mask'] == 1]\n",
    "tokenizer.decode(context_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `token_type_ids` key\n",
    "Differentiate two types of tokens, the ones that correspond to the question and the ones that correspond to the answers. More info [here](https://huggingface.co/transformers/glossary.html#token-type-ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the padding tokens by evaluating the train sample on\n",
    "# train_sample['attention_mask'] == 1\n",
    "train_sample['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the question tokens by evaluating the train sample on\n",
    "# train_sample['token_type_ids'] == 0\n",
    "paragraph_encoded = train_sample['input_ids'][train_sample['token_type_ids'] == 0]\n",
    "tokenizer.decode(paragraph_encoded,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the context tokens by evaluating the train sample on\n",
    "# train_sample['token_type_ids'] == 1\n",
    "question_encoded = train_sample['input_ids'][train_sample['token_type_ids'] == 1]\n",
    "tokenizer.decode(question_encoded, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
